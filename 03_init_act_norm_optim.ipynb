{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoA1Ny9xeHdk"
      },
      "source": [
        "## Инициализация и нормализация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHU-PG6beHdm"
      },
      "source": [
        "В этом задании вам предстоит реализовать два вида нормализации: по батчам (BatchNorm1d) и по признакам (LayerNorm1d)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QrCR5-YqeHdn"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, NamedTuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Iw1WtqeHdo"
      },
      "source": [
        "### 1. Реализация BatchNorm1d и LayerNorm1d."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFaPIHmteHdo"
      },
      "source": [
        "#### 1.1. (2 балла) Реализуйте BatchNorm1d\n",
        "\n",
        "Подсказка: чтобы хранить текущие значения среднего и дисперсии, вам потребуется метод `torch.nn.Module.register_buffer`, ознакомьтесь с документацией к нему. Подумайте, какие проблемы возникнут, если вы будете просто сохранять ваши значения в тензор.\n",
        "\n",
        "\n",
        "Важно помнить:\n",
        "- Понятно, что нормализацию мы добавляем после очередного слоя с параметрами, но до применения функции активации или после? Подумайте, есть ли у одного из этих способов преимущества над другим.\n",
        "- Модуль нормализации по батчам работает по-разному при обучении и при валидации, и ему нужно понимать, в каком он состоянии. Эта информация доступна в атрибуте модуля `self.training: bool`, его значение определит ветвление логики в вашей реализации метода `forward`.\n",
        "- Переключение модулей между режимами осуществляется вызовами у объекта модели методов `.train()` (переключение в режим обучения) и `.eval()` (переключение в режим валидации) Почитайте документацию к этим методам. Ваши функции `train_epoch` и `test_epoch` теперь должны переводить модель в нужный режим перед началом обработки данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QZwDVoS0eHdo"
      },
      "outputs": [],
      "source": [
        "class BatchNorm1d(nn.Module):\n",
        "    def __init__(\n",
        "        self, num_features: int, momentum: float = 0.9, eps: float = 1e-5\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.scale = nn.Parameter(torch.ones(num_features))\n",
        "        self.shift = nn.Parameter(torch.zeros(num_features))\n",
        "        self.register_buffer(\"running_mean\", torch.zeros(num_features))\n",
        "        self.register_buffer(\"running_var\", torch.ones(num_features))\n",
        "        self.momentum = momentum\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if self.training:\n",
        "            batch_mean = x.mean(dim=0)\n",
        "            batch_var = x.var(dim=0, unbiased=False)\n",
        "\n",
        "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
        "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
        "\n",
        "            mean = batch_mean\n",
        "            var = batch_var\n",
        "        else:\n",
        "            mean = self.running_mean\n",
        "            var = self.running_var\n",
        "\n",
        "        x_normalized = (x - mean) / torch.sqrt(var + self.eps)\n",
        "\n",
        "        out = self.scale * x_normalized + self.shift\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdCjPA85eHdp"
      },
      "source": [
        "#### 1.2. (1 балл) Реализуйте LayerNorm1d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mau_KOWAeHdp"
      },
      "source": [
        "Отличия LayerNorm от BatchNorm - в том, что расчёт средних и дисперсий в BatchNorm происходит вдоль размерности батча (см. рисунок слева), а в LayerNorm - вдоль размерности признаков (см. рисунок справа).\n",
        "\n",
        "<img src=\"../attachments/norm.png\" width=\"800\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ah64LhlweHdp"
      },
      "outputs": [],
      "source": [
        "class LayerNorm1d(nn.Module):\n",
        "    def __init__(self, num_features: int, eps: float = 1e-5) -> None:\n",
        "        super(LayerNorm1d, self).__init__()\n",
        "        self.scale = nn.Parameter(torch.ones(num_features))\n",
        "        self.shift = nn.Parameter(torch.zeros(num_features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        mean = x.mean(dim=1, keepdim=True)\n",
        "        var = x.var(dim=1, keepdim=True, unbiased=False)\n",
        "        x_normalized = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        out = self.scale * x_normalized + self.shift\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0APkmJU_eHdp"
      },
      "source": [
        "Замечание: явных тестов на корректность в этом задании нет, так как конкретные реализации могут давать немного разные результаты. Но вы можете проверить корректность своей реализации в эксперименте сами, нормализация должна немного исправлять проблемы неудачной инициализации. Ну и как минимум вы можете вручную проверить, что ваши активации действительно нормализуются в результате применения вашего модуля."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36AE5sD2eHdp"
      },
      "source": [
        "### 2. Эксперименты"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVrhYY_6eHdq"
      },
      "source": [
        "В этом задании ваша задача - проверить, какие из приёмов хорошо справляются с нездоровыми активациями в промежуточных слоях. Вам будет дана базовая модель, у которой есть проблемы с инициализацией параметров, попробуйте несколько приёмов для устранения проблем обучения:\n",
        "1. Хорошая инициализация параметров\n",
        "2. Ненасыщаемая функция активации (например, `F.leaky_relu`)\n",
        "3. Нормализация по батчам или по признакам (можно использовать встроенные `nn.BatchNorm1d` и `nn.LayerNorm`)\n",
        "4. Более продвинутый оптимизатор (`torch.optim.RMSprop`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb60QiS4eHdq"
      },
      "source": [
        "#### 2.0. Подготовка: датасет, функции для обучения"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDyTMPpTeHdq"
      },
      "source": [
        "Проверять наши гипотезы будем на датасете MNIST, для отладки добавим в функции для обучения возможность использовать только несколько батчей данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGg2fax1eHdq",
        "outputId": "f8af2ab8-cce3-42d0-e045-f34ef63a0125"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 125735912.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 23724186.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 96466710.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4215651.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    \"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor(),\n",
        ")\n",
        "test_dataset = datasets.MNIST(\n",
        "    \"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor(),\n",
        ")\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hMxAx1cseHdq"
      },
      "outputs": [],
      "source": [
        "def training_step(\n",
        "    batch: tuple[torch.Tensor, torch.Tensor],\n",
        "    model: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        ") -> torch.Tensor:\n",
        "    # прогоняем батч через модель\n",
        "    x, y = batch\n",
        "    logits = model(x)\n",
        "    # оцениваем значение ошибки\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    # обновляем параметры\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    # возвращаем значение функции ошибки для логирования\n",
        "    return loss\n",
        "\n",
        "\n",
        "def train_epoch(\n",
        "    dataloader: DataLoader,\n",
        "    model: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    max_batches: int = 100,\n",
        ") -> Tensor:\n",
        "    loss_values: list[float] = []\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        loss = training_step(batch, model, optimizer)\n",
        "        loss_values.append(loss.item())\n",
        "        if i == max_batches:\n",
        "            break\n",
        "    return torch.tensor(loss_values).mean()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_epoch(\n",
        "    dataloader: DataLoader, model: nn.Module, max_batches: int = 100\n",
        ") -> Tensor:\n",
        "    loss_values: list[float] = []\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        x, y = batch\n",
        "        logits = model(x)\n",
        "        # оцениваем значение ошибки\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        loss_values.append(loss.item())\n",
        "        if i == max_batches:\n",
        "            break\n",
        "    return torch.tensor(loss_values).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k6S5sFgeHdq"
      },
      "source": [
        "#### 2.1. Определение класса модели (2 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQo8sjIXeHdq"
      },
      "source": [
        "Для удобства проведения экспериментов мы немного усложним создание модели, чтобы можно было задать разные способы инициализации параметров и нормализации промежуточных активаций, не меняя определение класса.\n",
        "\n",
        "Добавьте в метод `__init__`:\n",
        "- аргумент, который позволит использовать разные функции активации для промежуточных слоёв\n",
        "- аргумент, который позволит задавать разные способы нормализации: `None` (без нормализации), `nn.BatchNorm` и `nn.LayerNorm`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "T2O_tjW5eHdq"
      },
      "outputs": [],
      "source": [
        "def init_std_normal(model: nn.Module) -> None:\n",
        "    \"\"\"Функция для инициализации параметров модели стандартным нормальным распределением.\"\"\"\n",
        "    for param in model.parameters():\n",
        "        torch.nn.init.normal_(param.data, mean=0, std=1)\n",
        "\n",
        "\n",
        "from typing import Type\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"Базовая модель для экспериментов\n",
        "\n",
        "    Args:\n",
        "        input_dim (int): размерность входных признаков\n",
        "        hidden_dim (int): размерност скрытого слоя\n",
        "        output_dim (int): кол-во классов\n",
        "        act_fn (Callable[[Tensor], Tensor], optional): Функция активации. Defaults to F.tanh.\n",
        "        init_fn (Callable[[nn.Module], None], optional): Функция для инициализации. Defaults to init_std_normal.\n",
        "        norm (Type[nn.BatchNorm1d  |  nn.LayerNorm] | None, optional): Способ нормализации промежуточных активаций.\n",
        "            Defaults to None.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dim: int,\n",
        "        output_dim: int,\n",
        "        act_fn: Callable[[Tensor], Tensor] = F.tanh,\n",
        "        init_fn: Callable[[nn.Module], None] = init_std_normal,\n",
        "        norm: Type[nn.BatchNorm1d | nn.LayerNorm] | None = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        # теперь линейные слои будем задавать\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        self.act_fn = act_fn\n",
        "        if norm is not None:\n",
        "            self.norm = norm(hidden_dim)\n",
        "        else:\n",
        "            self.norm = None\n",
        "\n",
        "        init_fn(self)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        h = self.fc1(x.flatten(1))\n",
        "        if self.norm is not None:\n",
        "            h = self.norm(h)\n",
        "        h = self.act_fn(h)\n",
        "        out = self.fc2(h)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_kZwftdeHdr"
      },
      "source": [
        "#### 2.2. Эксперименты (7 баллов)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OaR5uYgeHdr"
      },
      "source": [
        "Проведите по 3 эксперимента с каждой из модификаций с разными значениями `seed`, соберите статистику значений тестовой ошибки после 10 эпох обучения, сделайте выводы о том, что работает лучше\n",
        "\n",
        "Проверяем:\n",
        "1. Метод инициализации весов модели: $\\mathcal{N}(0, 1)$ / Kaiming normal\n",
        "2. Функция активации: tanh /  (или любая другая без насыщения)\n",
        "3. Слой нормализации: None / BatchNorm / LayerNorm\n",
        "4. Выбранный оптимизатор: SGD / RMSprop / Adam\n",
        "\n",
        "Итого у нас 7 экспериментов:\n",
        "- исходный (1)\n",
        "- смена инициализации (1)\n",
        "- смена нелинейности (1)\n",
        "- смена нормализации (2)\n",
        "- смена оптимизатора (2)\n",
        "\n",
        "Каждый эксперимент нужно повторить 3 раза с разными значениями random seed, посчитать среднее и вывести результаты в pandas.DataFrame.\n",
        "Можно дополнительно потестировать разные сочетания опций, например инициализация + нормализация\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm7eDdt1eHdr"
      },
      "source": [
        "Чтобы автоматизировать проведение экспериментов, можно использовать функцию, которая будет принимать все необходимые настройки эксперимента, запускать его и сохранять нужные метрики:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9I-cuWt6eHdr"
      },
      "outputs": [],
      "source": [
        "def run_experiment(\n",
        "    model_gen: Callable[[], nn.Module],\n",
        "    optim_gen: Callable[[nn.Module], torch.optim.Optimizer],\n",
        "    seed: int,\n",
        "    n_epochs: int = 10,\n",
        "    max_batches: int | None = None,\n",
        "    verbose: bool = False,\n",
        ") -> float:\n",
        "    \"\"\"Функция для запуска экспериментов.\n",
        "\n",
        "    Args:\n",
        "        model_gen (Callable[[], nn.Module]): Функция для создания модели\n",
        "        optim_gen (Callable[[nn.Module], torch.optim.Optimizer]): Функция для создания оптимизатора для модели\n",
        "        seed (int): random seed\n",
        "        n_epochs (int, optional): Число эпох обучения. Defaults to 10.\n",
        "        max_batches (int | None, optional): Если указано, только `max_batches` минибатчей\n",
        "            будет использоваться при обучении и тестировании. Defaults to None.\n",
        "        verbose (bool, optional): Выводить ли информацию для отладки. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        float: Значение ошибки на тестовой выборке в конце обучения\n",
        "    \"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    # создадим модель и выведем значение ошибки после инициализации\n",
        "    model = model_gen()\n",
        "    optim = optim_gen(model)\n",
        "    epoch_losses: list[float] = []\n",
        "    for i in range(n_epochs):\n",
        "        train_loss = train_epoch(train_loader, model, optim, max_batches=max_batches)\n",
        "        test_loss = test_epoch(test_loader, model, max_batches=max_batches)\n",
        "        if verbose:\n",
        "            print(f\"Epoch {i} train loss = {train_loss:.4f}\")\n",
        "            print(f\"Epoch {i} test loss = {test_loss:.4f}\")\n",
        "\n",
        "        epoch_losses.append(test_loss.item())\n",
        "\n",
        "    last_epoch_loss = epoch_losses[-1]\n",
        "    return last_epoch_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P7MX-x-eHdr"
      },
      "source": [
        "Пример использования:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5iWAoANeHdr",
        "outputId": "ae6f2980-320e-4852-a492-cb5bbcb9e63b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 train loss = 12.6168\n",
            "Epoch 0 test loss = 9.9327\n",
            "Epoch 1 train loss = 9.0954\n",
            "Epoch 1 test loss = 7.5498\n",
            "Epoch 2 train loss = 6.9607\n",
            "Epoch 2 test loss = 6.2342\n",
            "Epoch 3 train loss = 5.8992\n",
            "Epoch 3 test loss = 5.3655\n",
            "Epoch 4 train loss = 4.9951\n",
            "Epoch 4 test loss = 4.7433\n",
            "Epoch 5 train loss = 4.4778\n",
            "Epoch 5 test loss = 4.3001\n",
            "Epoch 6 train loss = 3.9693\n",
            "Epoch 6 test loss = 3.9605\n",
            "Epoch 7 train loss = 3.7261\n",
            "Epoch 7 test loss = 3.6844\n",
            "Epoch 8 train loss = 3.4223\n",
            "Epoch 8 test loss = 3.4538\n",
            "Epoch 9 train loss = 2.9975\n",
            "Epoch 9 test loss = 3.2638\n"
          ]
        }
      ],
      "source": [
        "losses = run_experiment(\n",
        "    model_gen=lambda: MLP(784, 128, 10, init_fn=init_std_normal, norm=None),\n",
        "    optim_gen=lambda x: torch.optim.SGD(x.parameters(), lr=0.01),\n",
        "    seed=42,\n",
        "    n_epochs=10,\n",
        "    max_batches=100,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp_HSXuneHds"
      },
      "source": [
        "Для удобства задания настроек эксперимента можно определять их с помощью класса `Experiment`, в котором можно также реализовать логику для строкового представления:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kKshmDXoeHds"
      },
      "outputs": [],
      "source": [
        "input_dim = 784\n",
        "hidden_dim = 128\n",
        "output_dim = len(train_dataset.classes)\n",
        "\n",
        "\n",
        "class Experiment(NamedTuple):\n",
        "    init_fn: Callable[[nn.Module], None]\n",
        "    act_fn: Callable[[Tensor], Tensor]\n",
        "    norm: Type[nn.BatchNorm1d | nn.LayerNorm] | None\n",
        "    optim_cls: Type[torch.optim.Optimizer]\n",
        "\n",
        "    @property\n",
        "    def model_gen(self) -> Callable[[], nn.Module]:\n",
        "        return lambda: MLP(\n",
        "            input_dim, hidden_dim, output_dim, init_fn=self.init_fn, norm=self.norm\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def optim_gen(self) -> Callable[[nn.Module], torch.optim.Optimizer]:\n",
        "        return lambda x: self.optim_cls(x.parameters(), lr=0.01)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Experiment(init_fn={self.init_fn.__name__}, act_fn={self.act_fn.__name__}, norm={self.norm.__name__ if self.norm else 'None'}, optim_cls={self.optim_cls.__name__})\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zwQSCq_eHds"
      },
      "source": [
        "Описываем все эксперименты:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBTIOvqmeHds",
        "outputId": "f4073e53-4ad4-40c1-c181-8b2b2413084c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Experiment(init_fn=init_std_normal, act_fn=tanh, norm=None, optim_cls=SGD),\n",
              " Experiment(init_fn=init_std_normal, act_fn=silu, norm=LayerNorm, optim_cls=SGD),\n",
              " Experiment(init_fn=init_std_normal, act_fn=relu, norm=BatchNorm1d, optim_cls=RMSprop),\n",
              " Experiment(init_fn=init_kaiming_normal, act_fn=tanh, norm=None, optim_cls=SGD),\n",
              " Experiment(init_fn=init_std_normal, act_fn=relu, norm=None, optim_cls=SGD),\n",
              " Experiment(init_fn=init_std_normal, act_fn=tanh, norm=BatchNorm1d, optim_cls=SGD),\n",
              " Experiment(init_fn=init_std_normal, act_fn=tanh, norm=None, optim_cls=Adam)]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def init_kaiming_normal(model: nn.Module) -> None:\n",
        "    \"\"\"Функция для инициализации параметров модели с помощью Kaiming normal.\"\"\"\n",
        "    for param in model.parameters():\n",
        "        if param.dim() > 1:  # Применяем инициализацию только к весам, а не к смещениям\n",
        "            torch.nn.init.kaiming_normal_(param)\n",
        "\n",
        "\n",
        "options = [\n",
        "    Experiment(\n",
        "        init_fn=init_std_normal,\n",
        "        act_fn=F.tanh,\n",
        "        norm=None,\n",
        "        optim_cls=torch.optim.SGD,\n",
        "    ),\n",
        "    Experiment(\n",
        "        init_fn=init_std_normal,\n",
        "        act_fn=F.silu,\n",
        "        norm=nn.LayerNorm,\n",
        "        optim_cls=torch.optim.SGD,\n",
        "    ),\n",
        "    Experiment(\n",
        "        init_fn=init_std_normal,\n",
        "        act_fn=F.relu,\n",
        "        norm=nn.BatchNorm1d,\n",
        "        optim_cls=torch.optim.RMSprop,\n",
        "    ),\n",
        "    Experiment(\n",
        "        init_fn=init_kaiming_normal,\n",
        "        act_fn=F.tanh,\n",
        "        norm=None,\n",
        "        optim_cls=torch.optim.SGD,\n",
        "    ),\n",
        "    Experiment(\n",
        "        init_fn=init_std_normal,\n",
        "        act_fn=F.relu,\n",
        "        norm=None,\n",
        "        optim_cls=torch.optim.SGD,\n",
        "    ),\n",
        "    Experiment(\n",
        "        init_fn=init_std_normal,\n",
        "        act_fn=F.tanh,\n",
        "        norm=nn.BatchNorm1d,\n",
        "        optim_cls=torch.optim.SGD,\n",
        "    ),\n",
        "    Experiment(\n",
        "        init_fn=init_std_normal,\n",
        "        act_fn=F.tanh,\n",
        "        norm=None,\n",
        "        optim_cls=torch.optim.Adam,\n",
        "    ),\n",
        "\n",
        "]\n",
        "\n",
        "options"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7tWQNTBeHds"
      },
      "source": [
        "Запускаем расчёты:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M75R4vkeHds",
        "outputId": "b80b25e6-c140-40f6-b1ef-cc241b63c79b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running experiment: Experiment(init_fn=init_std_normal, act_fn=tanh, norm=None, optim_cls=SGD)\n",
            "Running experiment: Experiment(init_fn=init_std_normal, act_fn=silu, norm=LayerNorm, optim_cls=SGD)\n",
            "Running experiment: Experiment(init_fn=init_std_normal, act_fn=relu, norm=BatchNorm1d, optim_cls=RMSprop)\n",
            "Running experiment: Experiment(init_fn=init_kaiming_normal, act_fn=tanh, norm=None, optim_cls=SGD)\n",
            "Running experiment: Experiment(init_fn=init_std_normal, act_fn=relu, norm=None, optim_cls=SGD)\n",
            "Running experiment: Experiment(init_fn=init_std_normal, act_fn=tanh, norm=BatchNorm1d, optim_cls=SGD)\n",
            "Running experiment: Experiment(init_fn=init_std_normal, act_fn=tanh, norm=None, optim_cls=Adam)\n"
          ]
        }
      ],
      "source": [
        "seeds = [42, 43, 44]  # используем три разных значения seed\n",
        "results = []\n",
        "\n",
        "for option in options:  # используем список экспериментов\n",
        "    print(f\"Running experiment: {option}\")\n",
        "    for seed in seeds:\n",
        "        loss = run_experiment(\n",
        "            model_gen=option.model_gen,  # генерируем модель\n",
        "            optim_gen=option.optim_gen,  # генерируем оптимизатор\n",
        "            seed=seed,\n",
        "            n_epochs=10,\n",
        "            max_batches=None,\n",
        "            verbose=False,\n",
        "        )\n",
        "        results.append([str(option), seed, loss])\n",
        "\n",
        "# Для удобного вывода результатов используем pandas\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx3cP1hkeHds"
      },
      "source": [
        "Выводим результаты:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-gmnZw-eHds",
        "outputId": "c7eb26f7-180b-4ae3-9892-0f659018a8a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment\n",
            "Experiment(init_fn=init_kaiming_normal, act_fn=tanh, norm=None, optim_cls=SGD)           0.194905\n",
            "Experiment(init_fn=init_std_normal, act_fn=relu, norm=BatchNorm1d, optim_cls=RMSprop)    0.157090\n",
            "Experiment(init_fn=init_std_normal, act_fn=relu, norm=None, optim_cls=SGD)               0.673810\n",
            "Experiment(init_fn=init_std_normal, act_fn=silu, norm=LayerNorm, optim_cls=SGD)          0.460582\n",
            "Experiment(init_fn=init_std_normal, act_fn=tanh, norm=BatchNorm1d, optim_cls=SGD)        0.481511\n",
            "Experiment(init_fn=init_std_normal, act_fn=tanh, norm=None, optim_cls=Adam)              0.193152\n",
            "Experiment(init_fn=init_std_normal, act_fn=tanh, norm=None, optim_cls=SGD)               0.673810\n",
            "Name: Test Loss, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df_results = pd.DataFrame(results, columns=[\"Experiment\", \"Seed\", \"Test Loss\"])\n",
        "\n",
        "mean_results = df_results.groupby(\"Experiment\").mean(numeric_only=True)[\"Test Loss\"]\n",
        "print(mean_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYzS2hfNeHdt"
      },
      "source": [
        "ВЫВОДЫ:\n",
        "Лучшие результаты дали комбинации с kaiming_normal и BatchNorm1d в паре с оптимизатором RMSprop. В то же время отсутствие нормализации и использование оптимизатора SGD сильно ухудшили обучение. Нормализация и продвинутая инициализация весов заметно улучшают качество модели."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dl-mcs",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
